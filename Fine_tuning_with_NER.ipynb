{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Fine tuning with NER",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshara-Balan/Finetuning-with-NER/blob/main/Fine_tuning_with_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install transformers datasets seqeval torch accelerate\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T06:43:05.860267Z",
          "iopub.execute_input": "2025-02-19T06:43:05.860585Z",
          "iopub.status.idle": "2025-02-19T06:43:21.060752Z",
          "shell.execute_reply.started": "2025-02-19T06:43:05.860562Z",
          "shell.execute_reply": "2025-02-19T06:43:21.059711Z"
        },
        "id": "IvmKhRk3YOIN",
        "outputId": "2b092832-7cee-4575-8835-013b734e81fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=a6b1809fcc1f02814782b6881ec4a9f1647e8bf49dc0b0a0711f12da877979bc\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def setup_training(model_name=\"bert-base-multilingual-cased\", batch_size=4):\n",
        "    # Load the dataset without streaming\n",
        "    try:\n",
        "        dataset = load_dataset(\"ai4bharat/naamapadam\", \"ml\")\n",
        "        # Take a subset of the dataset\n",
        "        subset_size = 1000  # Adjust this number based on your memory constraints\n",
        "        dataset['train'] = Dataset.from_dict(\n",
        "            dataset['train'][:subset_size]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Load tokenizer with lower memory footprint\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    # Create label mapping\n",
        "    unique_labels = set()\n",
        "    for example in dataset['train']:\n",
        "        unique_labels.update(example[\"ner_tags\"])\n",
        "    label_list = sorted(list(unique_labels))\n",
        "    label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "    id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "    def tokenize_and_align_labels(examples, max_length=128):  # Reduced from 512\n",
        "        try:\n",
        "            tokenized_inputs = tokenizer(\n",
        "                examples[\"tokens\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                is_split_into_words=True,\n",
        "                return_overflowing_tokens=True,\n",
        "            )\n",
        "\n",
        "            sample_map = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
        "            all_new_labels = []\n",
        "\n",
        "            for i in range(len(tokenized_inputs[\"input_ids\"])):\n",
        "                sample_index = sample_map[i]\n",
        "                label = examples[\"ner_tags\"][sample_index]\n",
        "\n",
        "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "                previous_word_idx = None\n",
        "                new_labels = []\n",
        "\n",
        "                for word_idx in word_ids:\n",
        "                    if word_idx is None:\n",
        "                        new_labels.append(-100)\n",
        "                    elif word_idx != previous_word_idx:\n",
        "                        if word_idx < len(label):\n",
        "                            new_labels.append(label_to_id.get(label[word_idx], 0))\n",
        "                        else:\n",
        "                            new_labels.append(0)\n",
        "                        previous_word_idx = word_idx\n",
        "                    else:\n",
        "                        new_labels.append(-100)\n",
        "\n",
        "                all_new_labels.append(new_labels)\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = all_new_labels\n",
        "            return tokenized_inputs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in tokenization: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Process dataset with memory-efficient batching\n",
        "    tokenized_dataset = dataset['train'].map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        batch_size=32,  # Smaller batch size for processing\n",
        "        remove_columns=['tokens', 'ner_tags']\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_test_ratio = 0.2\n",
        "    dataset_dict = tokenized_dataset.train_test_split(test_size=train_test_ratio, seed=42)\n",
        "    train_dataset = dataset_dict['train']\n",
        "    validation_dataset = dataset_dict['test']\n",
        "\n",
        "    # Clear memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize model with gradient checkpointing\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_list)\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
        "\n",
        "    # Training arguments optimized for memory efficiency\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./ner_results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        report_to=[],\n",
        "        run_name=None,\n",
        "        gradient_accumulation_steps=4,  # Reduce memory usage\n",
        "        fp16=True,  # Use mixed precision training\n",
        "        optim=\"adamw_torch\",\n",
        "    )\n",
        "\n",
        "    return model, tokenizer, train_dataset, validation_dataset, training_args, label_list, id_to_label\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "def train_model(model, tokenizer, train_dataset, validation_dataset, training_args, id_to_label):\n",
        "    # Metric computation\n",
        "    metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        try:\n",
        "            logits, labels = eval_pred\n",
        "            predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "            true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]\n",
        "            true_predictions = [[id_to_label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "                              for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "            results = metric.compute(predictions=true_predictions, references=true_labels, scheme=\"plain\")\n",
        "            return {\n",
        "                \"precision\": results[\"overall_precision\"],\n",
        "                \"recall\": results[\"overall_recall\"],\n",
        "                \"f1\": results[\"overall_f1\"],\n",
        "                \"accuracy\": results[\"overall_accuracy\"],\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Metric computation error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=validation_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "        print(\"Training completed successfully\")\n",
        "\n",
        "        print(\"Saving model...\")\n",
        "        trainer.save_model(\"./fine_tuned_malayalam_ner\")\n",
        "        tokenizer.save_pretrained(\"./fine_tuned_malayalam_ner\")\n",
        "        print(\"Model saved successfully\")\n",
        "\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Start with a smaller batch size\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Setup training components\n",
        "    components = setup_training(batch_size=BATCH_SIZE)\n",
        "    if components is None:\n",
        "        return\n",
        "\n",
        "    model, tokenizer, train_dataset, validation_dataset, training_args, label_list, id_to_label = components\n",
        "\n",
        "    # Train model\n",
        "    trainer = train_model(model, tokenizer, train_dataset, validation_dataset, training_args, id_to_label)\n",
        "\n",
        "    if trainer is not None:\n",
        "        print(\"Training completed successfully\")\n",
        "    else:\n",
        "        print(\"Training failed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T06:43:21.062356Z",
          "iopub.execute_input": "2025-02-19T06:43:21.062711Z",
          "execution_failed": "2025-02-19T06:43:57.166Z"
        },
        "id": "oh_rDqSxYOIN",
        "outputId": "12b98820-d209-47f7-929d-4624642c2e57",
        "colab": {
          "referenced_widgets": [
            "08ee12975f1c4902ae2a1f2c05cfa8fe",
            "8466c46c0aae40a38c558ce173cfa8c2",
            "0eb34f27f28d4d849cf8d080816a511d",
            "706e51aabb254cf5941b24a91bc8e944",
            "48c4ada0185a42c2b865f542f6912ce4",
            "0fa8f7eea85b44fe9146330298bde75f",
            "e301fa90b5464bdba68d9a67bb3b699f",
            "b427e58ad2df4985bb1f50eef61691d7",
            "9198583f87ca4654b1f3d6007338257d",
            "d48c032e5d424b3eb0e8f863f4467193",
            "a2a80e8c39a04e7b923a5fc3ca6721fe",
            "a4953c89f547484d89917a468693c1b0",
            "83cee45640c04739949bbe4a7516976a",
            "1e50dcc824e84f269fae439691fbe27a",
            "01d105747d5d40deaabcef9ceee9952f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/8.65k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08ee12975f1c4902ae2a1f2c05cfa8fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "naamapadam.py:   0%|          | 0.00/2.86k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8466c46c0aae40a38c558ce173cfa8c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0000.parquet:   0%|          | 0.00/61.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eb34f27f28d4d849cf8d080816a511d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0000.parquet:   0%|          | 0.00/81.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "706e51aabb254cf5941b24a91bc8e944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0000.parquet:   0%|          | 0.00/315k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48c4ada0185a42c2b865f542f6912ce4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/716652 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fa8f7eea85b44fe9146330298bde75f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/974 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e301fa90b5464bdba68d9a67bb3b699f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/3618 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b427e58ad2df4985bb1f50eef61691d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9198583f87ca4654b1f3d6007338257d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d48c032e5d424b3eb0e8f863f4467193"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2a80e8c39a04e7b923a5fc3ca6721fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4953c89f547484d89917a468693c1b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83cee45640c04739949bbe4a7516976a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e50dcc824e84f269fae439691fbe27a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01d105747d5d40deaabcef9ceee9952f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Starting training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_examples(examples=5):\n",
        "    print(\"\\nTesting model with examples:\")\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\"./fine_tuned_malayalam_ner\")\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    for i in range(min(examples, len(dataset[\"train\"]))):\n",
        "        example = dataset[\"train\"][i]\n",
        "\n",
        "        # Ensure the sentence is passed as a string\n",
        "        sentence = example[\"sentence\"]\n",
        "        if isinstance(sentence, list):\n",
        "            sentence = \" \".join(sentence)  # Convert list to a string if needed\n",
        "\n",
        "        tokens = tokenizer(sentence,\n",
        "                          return_tensors=\"pt\",\n",
        "                          truncation=True,\n",
        "                          is_split_into_words=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens)\n",
        "            predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "        token_predictions = [id_to_label[p.item()] for p in predictions[0]]\n",
        "\n",
        "        # Align predictions with original words\n",
        "        word_ids = tokens.word_ids()\n",
        "        aligned_predictions = []\n",
        "        for word_idx in range(len(sentence.split())):\n",
        "            word_predictions = [token_predictions[j] for j, w_id in enumerate(word_ids) if w_id == word_idx]\n",
        "            if word_predictions:\n",
        "                aligned_predictions.append(max(set(word_predictions), key=word_predictions.count))\n",
        "            else:\n",
        "                aligned_predictions.append(\"O\")  # Default if no prediction\n",
        "\n",
        "        print(f\"Example {i}:\")\n",
        "        print(\"Sentence:\", sentence)\n",
        "        print(\"True tags:\", [id_to_label.get(tag, \"Unknown\") for tag in example[\"ner_tags\"]])\n",
        "        print(\"Predicted:\", aligned_predictions[:len(sentence.split())])\n",
        "        print()\n",
        "\n",
        "# Uncomment to test the model\n",
        "import torch\n",
        "test_model_with_examples()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "lNok4bPWYOIO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "s2a9Xy4zYOIP"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}